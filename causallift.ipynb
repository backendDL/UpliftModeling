{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "from typing import Optional, Union, Tuple, List\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import causallift\n",
    "from causallift import CausalLift\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "causallift.__version__\n",
    "pd.options.display.max_rows = 8\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data: str, **kwargs):\n",
    "\n",
    "    if data == 'simulated_observational_data':\n",
    "        \"\"\"\n",
    "        # Generate simulated data\n",
    "        # \"Sleeping dogs\" (a.k.a. \"do-not-disturb\"; people who will \"buy\" if not \n",
    "        treated but will not \"buy\" if treated) can be simulated by negative values \n",
    "        in tau parameter.\n",
    "        # Observational data which includes confounding can be simulated by \n",
    "        non-zero values in propensity_coef parameter.  \n",
    "        # A/B Test (RCT) with a 50:50 split can be simulated by all-zeros values \n",
    "        in propensity_coef parameter (default).\n",
    "        # The first element in each list parameter specifies the intercept.\n",
    "        \"\"\"\n",
    "        from causallift import generate_data\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        df = generate_data(\n",
    "            N=20000, \n",
    "            n_features=10, \n",
    "            beta=np.random.uniform(low=-3.0, high=5.0, size=10+1).tolist(), # Effect of [intercept and features] on outcome \n",
    "            error_std=0.1, \n",
    "            tau=np.random.uniform(low=-1.0, high=1.0, size=10+1).tolist(), # Effect of [intercept and features] on treated outcome\n",
    "            tau_std=0.1, \n",
    "            discrete_outcome=True, \n",
    "            seed=seed, \n",
    "            feature_effect=0, # Effect of beta on treated outcome\n",
    "            propensity_coef=np.random.randint(low=-1, high=1, size=10+1).tolist(), # Effect of [intercept and features] on propensity log-odds for treatment\n",
    "            index_name='index',\n",
    "        )\n",
    "        \n",
    "    elif data == 'lalonde':\n",
    "        r\"\"\" \n",
    "            Lalonde dataset was used to evaluate propensity score in the paper:\n",
    "            Dehejia, R., & Wahba, S. (1999). Causal Effects in Nonexperimental \n",
    "            Studies: Reevaluating the Evaluation of Training Programs. Journal of \n",
    "            the American Statistical Association, 94(448), 1053-1062. \n",
    "            doi:10.2307/2669919\n",
    "\n",
    "            Lalonde dataset is now included in R package named \"Matching.\"\n",
    "            http://sekhon.berkeley.edu/matching/lalonde.html\n",
    "        \"\"\"\n",
    "        def get_lalonde():\n",
    "            r\"\"\" Load datasets, concatenate, and create features to get data frame \n",
    "            similar to 'lalonde' that comes with \"Matching.\")\n",
    "            \"\"\"\n",
    "            cols = ['treat', 'age', 'educ', 'black', 'hisp', 'married', 'nodegr','re74','re75','re78']\n",
    "            control_df = pd.read_csv('http://www.nber.org/~rdehejia/data/nswre74_control.txt', sep=r'\\s+', header = None, names = cols)\n",
    "            treated_df = pd.read_csv('http://www.nber.org/~rdehejia/data/nswre74_treated.txt', sep=r'\\s+', header = None, names = cols)\n",
    "            lalonde_df = pd.concat([control_df, treated_df], ignore_index=True)\n",
    "            lalonde_df['u74'] = np.where(lalonde_df['re74'] == 0, 1.0, 0.0)\n",
    "            lalonde_df['u75'] = np.where(lalonde_df['re75'] == 0, 1.0, 0.0)\n",
    "            return lalonde_df\n",
    "        lalonde_df = get_lalonde()\n",
    "        \n",
    "        \"\"\" Prepare the input Data Frame. \"\"\"\n",
    "        df = lalonde_df.copy()\n",
    "        df.rename(columns={'treat':'Treatment', 're78':'Outcome'}, inplace=True)\n",
    "        df['Outcome'] = np.where(df['Outcome'] > 0, 1.0, 0.0)\n",
    "        \n",
    "        # categorize age to 20s, 30s, 40s, and 50s and then one-hot encode age\n",
    "        df.loc[:,'age'] = df['age'].apply(lambda x:'{:.0f}'.format(x)[:-1]+'0s') \n",
    "        df = pd.get_dummies(df, columns=['age'], drop_first=True) \n",
    "        \n",
    "        cols = ['nodegr', 'black', 'hisp', 'age_20s', 'age_30s', 'age_40s', 'age_50s', \n",
    "                'educ', 'married', 'u74', 'u75', 'Treatment', 'Outcome']\n",
    "        df = df[cols]\n",
    "\n",
    "    elif data == 'criteo':\n",
    "        save_dir = \"./raw_data\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        criteo_url = \"http://go.criteo.net/criteo-research-uplift-v2.1.csv.gz\"\n",
    "        zip_file_name = \"raw_data/criteo-research-uplift-v2.1.csv.gz\"\n",
    "        unzip_file_name = \"raw_data/criteo-research-uplift-v2.1.csv\"\n",
    "        \n",
    "        if os.path.isfile(unzip_file_name):\n",
    "            print(\"The downloaded file already exists!\")\n",
    "        \n",
    "        else:\n",
    "            print(\"Try to download the raw data from the server...\")\n",
    "            response = requests.get(criteo_url, stream=True)\n",
    "            total_size_in_bytes = int(response.headers.get(\"content-length\", 0))\n",
    "            block_size = 1024\n",
    "            progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "            with open(zip_file_name, \"wb\") as f:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    progress_bar.update(len(data))\n",
    "                    f.write(data)\n",
    "            progress_bar.close()\n",
    "            print(\"Finished downloading!!!\")\n",
    "            if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "                print(\"Error, something went wrong!\")\n",
    "                return\n",
    "\n",
    "            print(\"Try to unzip the downloaded file\")\n",
    "            with gzip.open(zip_file_name, \"rb\") as f_in:\n",
    "                with open (unzip_file_name, \"wb\") as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "            os.remove(zip_file_name)\n",
    "            print(\"Zip file removed from disk\")\n",
    "\n",
    "        print(\"Import the csv file into pd.DataFrame\")\n",
    "        df = pd.read_csv(unzip_file_name)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"No corresponding data found\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(\"simulated_observational_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df[\"Treatment\"] == 1)/len(df)\n",
    "# 15% is treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(len(train_df), len(eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(corr, cmap=\"bwr\")\n",
    "plt.colorbar()\n",
    "plt.clim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df[\"Treatment\"], train_df[\"Outcome\"])/len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, column in enumerate(df.columns):\n",
    "    if idx >= len(df.columns) - 2:\n",
    "        # t and y varaibles\n",
    "        continue\n",
    "    plt.hist(df[column], density=True, histtype=\"step\", label=column)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpliftDataset(Dataset): \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame, \n",
    "        in_features: int,\n",
    "        t_idx: Optional[int] = None, \n",
    "        y_idx: Optional[int] = None,\n",
    "    ):\n",
    "        t_idx = in_features if t_idx is None else t_idx\n",
    "        y_idx = in_features + 1 if y_idx is None else y_idx\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.t_idx = t_idx\n",
    "        self.y_idx = y_idx\n",
    "\n",
    "        self.df = df\n",
    "        self.X = self.df.iloc[:, 0:in_features]\n",
    "        self.t = self.df.iloc[:, t_idx]\n",
    "        self.y = self.df.iloc[:, y_idx]\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X.iloc[idx, :].to_numpy(), dtype=torch.float32)\n",
    "        t = torch.tensor(self.t.iloc[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y.iloc[idx], dtype=torch.float32)\n",
    "        return (X, t, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 10\n",
    "\n",
    "train_set = UpliftDataset(train_df, num_features)\n",
    "eval_set  = UpliftDataset(eval_df, num_features)\n",
    "\n",
    "train_dl = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "eval_dl  = DataLoader(eval_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexModel(nn.Module): \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.expand = nn.Linear(in_features+1, 128)\n",
    "        self.fc1 = nn.Linear(128, 128) \n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 128) \n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.classifier = nn.Linear(128, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.expand(x)\n",
    "        _x = torch.tanh(self.bn1(self.fc1(x))) \n",
    "        x = self.dropout(_x) + x\n",
    "        _x = torch.tanh(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(_x) + x\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]\n",
    "class UpliftWrapper(nn.Module): \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "    ):\n",
    "        super(UpliftWrapper, self).__init__() \n",
    "        self.in_features = in_features \n",
    "        self.model = ComplexModel(in_features) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        t: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"Forward function for Uplift model\n",
    "        Args:\n",
    "            x: `torch.Tensor`\n",
    "            t: `torch.Tensor`\n",
    "        Returns:\n",
    "            `dict[str, torch.Tensor]`\n",
    "        \"\"\"\n",
    "        # X shape: (B, N)\n",
    "        if t.ndim == 2:\n",
    "            t = t.squeeze()\n",
    "        B = x.size(0)\n",
    "        L = x.size(1)\n",
    "        # print(f\"input shape: {x.shape}\")\n",
    "        \n",
    "        # first creating the inputs accordingly\n",
    "        x_0 = torch.cat([x, torch.zeros([B, 1]).to(x.device)], dim=1) \n",
    "        x_1 = torch.cat([x, torch.ones([B, 1]).to(x.device)], dim=1)\n",
    "\n",
    "        y_0 = self.sigmoid(self.model(x_0)).squeeze()\n",
    "        y_1 = self.sigmoid(self.model(x_1)).squeeze()\n",
    "        \n",
    "        pred = torch.where(t == 1, y_1, y_0)\n",
    "        return {\n",
    "            \"uplift\": y_1 - y_0, \"pred\": pred,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectUpliftLoss(nn.Module): \n",
    "    def __init__(self,\n",
    "        propensity_score: float = 0.5,\n",
    "        alpha: Optional[float] = None, \n",
    "    ):\n",
    "        super(DirectUpliftLoss, self).__init__()\n",
    "        if alpha > 1 or alpha < 0:\n",
    "            raise ValueError(\"alpha must be in [0, 1]\")\n",
    "        self.e_x = propensity_score \n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.loss_u = nn.MSELoss() \n",
    "        self.loss_y = nn.BCELoss()\n",
    "\n",
    "    def forward(self, out, t, y):\n",
    "        z = t * y / self.e_x - (1-t) * y / (1-self.e_x) \n",
    "        # variable transformation\n",
    "        \n",
    "        loss_uplift = self.loss_u(out[\"uplift\"], z) \n",
    "        loss_pred = self.loss_y(out[\"pred\"], y)\n",
    "        loss = (1-self.alpha) * loss_uplift + self.alpha * loss_pred \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UpliftWrapper(10)\n",
    "criterion = DirectUpliftLoss(0.5, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0][0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out = model(train_set[0][0].unsqueeze(0), train_set[0][1])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t, y = next(iter(train_dl))\n",
    "out = model(X, t)\n",
    "# print(out)\n",
    "# print(out[\"uplift\"].shape, out[\"pred\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"pred\"].shape, out[\"uplift\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].hist(out[\"pred\"].detach().cpu().numpy())\n",
    "ax[1].hist(out[\"uplift\"].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = criterion(out, t, y)\n",
    "loss = criterion(out, t, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = t * y / 0.5 - (1-t) * y / (1-0.5)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "eval_losses = []\n",
    "eval_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "\n",
    "    train_cnt = 0\n",
    "    train_corrects = 0\n",
    "\n",
    "    model.train()\n",
    "    for X, t, y in tqdm(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        train_cnt += X.size(0)\n",
    "\n",
    "        X = X.cuda()\n",
    "        t = t.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        out = model(X, t)\n",
    "        loss = criterion(out, t, y)\n",
    "        # loss = criterion(out[\"pred\"], y)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = np.where(out[\"pred\"].detach().cpu().numpy() > 0.5, 1, 0)\n",
    "        train_corrects += np.sum(pred == y.cpu().numpy())\n",
    "\n",
    "    train_accuracies.append(train_corrects/train_cnt)\n",
    "    print(f\"train accuracy: {train_accuracies[-1]}\")\n",
    "\n",
    "    eval_cnt = 0\n",
    "    eval_corrects = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, t, y in tqdm(eval_dl):\n",
    "            eval_cnt += X.size(0)\n",
    "\n",
    "            X = X.cuda()\n",
    "            t = t.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            out = model(X, t)\n",
    "            loss = criterion(out, t, y)\n",
    "            # loss = criterion(out[\"pred\"], y)\n",
    "            eval_losses.append(loss.item())\n",
    "\n",
    "            pred = np.where(out[\"pred\"].detach().cpu().numpy() > 0.5, 1, 0)\n",
    "            eval_corrects += np.sum(pred == y.cpu().numpy())\n",
    "\n",
    "    eval_accuracies.append(eval_corrects/eval_cnt)\n",
    "    print(f\"train accuracy: {eval_accuracies[-1]}\")\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(out[\"pred\"].detach().cpu().numpy(), bins=np.arange(0, 1.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"uplift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
